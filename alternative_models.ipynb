{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT VisionEncoder\n",
    "\n",
    "from transformers import ViTModel\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, d_out: int) -> None:\n",
    "        super().__init__()\n",
    "        self.base = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        d_in = self.base.config.hidden_size\n",
    "        self.projection = Projection(d_in, d_out)\n",
    "        \n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.base(pixel_values=x)\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]\n",
    "        projected_vec = self.projection(cls_token)\n",
    "        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n",
    "        return projected_vec / projection_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficientnet VisionEncoder\n",
    "\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, d_out: int) -> None:\n",
    "        super().__init__()\n",
    "        self.base = efficientnet_b0(pretrained=True)\n",
    "        d_in = self.base.classifier[1].in_features\n",
    "        self.base.classifier = nn.Identity()\n",
    "\n",
    "        self.projection = Projection(d_in, d_out)\n",
    "\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.base(x)\n",
    "        projected_vec = self.projection(features)\n",
    "        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n",
    "        return projected_vec / projection_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 TextEncoder\n",
    "\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, d_out: int) -> None:\n",
    "        super().__init__()\n",
    "        self.base = GPT2Model.from_pretrained(\"gpt2\")\n",
    "        d_in = self.base.config.hidden_size\n",
    "\n",
    "        self.projection = Projection(d_in, d_out)\n",
    "\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.base(input_ids=x)[0]  # pass only input_ids\n",
    "        out = out[:, -1, :]  # get last token output\n",
    "        projected_vec = self.projection(out)\n",
    "        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n",
    "        return projected_vec / projection_len\n",
    "    \n",
    "class Tokenizer:\n",
    "    def __init__(self, tokenizer: GPT2Tokenizer) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def __call__(self, x: str) -> GPT2Tokenizer:\n",
    "        return self.tokenizer(\n",
    "            x, truncation=True, padding=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, lr: float = 1e-3) -> None:\n",
    "        super().__init__()\n",
    "        self.vision_encoder = VisionEncoder(EMBED_DIM)\n",
    "        self.caption_encoder = TextEncoder(EMBED_DIM)\n",
    "        self.tokenizer = Tokenizer(GPT2Tokenizer.from_pretrained(\"gpt2\"))\n",
    "        self.lr = lr\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def forward(self, images, text):\n",
    "        text_tokens = self.tokenizer(text)\n",
    "        text_input_ids = text_tokens[\"input_ids\"].squeeze(1).to(self.device)  # Ensure correct shape\n",
    "\n",
    "        image_embed = self.vision_encoder(images)\n",
    "        caption_embed = self.caption_encoder(text_input_ids)\n",
    "\n",
    "        similarity = caption_embed @ image_embed.T\n",
    "\n",
    "        loss = self.CLIP_loss(similarity)\n",
    "        img_acc, cap_acc = metrics(similarity)\n",
    "\n",
    "        return loss, img_acc, cap_acc\n",
    "    \n",
    "    def CLIP_loss(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        n = logits.shape[1]      # number of samples\n",
    "        labels = torch.arange(n).to(self.device) # Create labels tensor\n",
    "        # Calculate cross entropy losses along axis 0 and 1\n",
    "        loss_i = F.cross_entropy(logits.transpose(0, 1), labels, reduction=\"mean\")\n",
    "        loss_t = F.cross_entropy(logits, labels, reduction=\"mean\")\n",
    "        # Calculate the final loss\n",
    "        loss = (loss_i + loss_t) / 2\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def top_image(self, images, text):\n",
    "        text = self.tokenizer(text).to(self.device)\n",
    "        caption_embed = self.caption_encoder(text[\"input_ids\"])\n",
    "\n",
    "        similarities = []\n",
    "\n",
    "        for image in images:\n",
    "            image_embed = self.vision_encoder(image.to(self.device))\n",
    "            similarities.append(F.cosine_similarity(image_embed, caption_embed, dim=1).item())\n",
    "\n",
    "        #top_image = np.argsort(similarities)[-1:][::-1]\n",
    "\n",
    "        return similarities\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = bert_tokenizer.eos_token\n",
    "\n",
    "def tokens(texts):\n",
    "    return tokenizer(texts, truncation=True, padding=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
