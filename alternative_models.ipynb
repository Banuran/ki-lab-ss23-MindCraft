{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, d_out: int) -> None:\n",
    "        super().__init__()\n",
    "        self.base = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        d_in = self.base.config.hidden_size\n",
    "        self.projection = Projection(d_in, d_out)\n",
    "        \n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.base(pixel_values=x)\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]\n",
    "        projected_vec = self.projection(cls_token)\n",
    "        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n",
    "        return projected_vec / projection_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, d_out: int) -> None:\n",
    "        super().__init__()\n",
    "        self.base = efficientnet_b0(pretrained=True)\n",
    "        d_in = self.base.classifier[1].in_features\n",
    "        self.base.classifier = nn.Identity()\n",
    "\n",
    "        self.projection = Projection(d_in, d_out)\n",
    "\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.base(x)\n",
    "        projected_vec = self.projection(features)\n",
    "        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n",
    "        return projected_vec / projection_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEHLERHAFT\n",
    "# Tokenizer müssen dafür überall angepasst werden\n",
    "\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, d_out: int) -> None:\n",
    "        super().__init__()\n",
    "        self.text_model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "        d_in = self.text_model.config.hidden_size\n",
    "\n",
    "        self.projection = Projection(d_in, d_out)\n",
    "\n",
    "        for p in self.text_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()} \n",
    "\n",
    "        outputs = self.text_model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        last_token = last_hidden_states[:, -1, :]\n",
    "\n",
    "        projected_vec = self.projection(last_token)\n",
    "\n",
    "        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n",
    "        return projected_vec / projection_len"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
