{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import pandas as pd\n",
    "import dataset as wsd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EMBED_DIM = 512\n",
    "TRANSFORMER_EMBED = 768\n",
    "IMAGE_SIZE = 255\n",
    "\n",
    "class Projection(nn.Module):\n",
    "    def __init__(self, d_in: int, d_out: int, p: float=0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.linear2 = nn.Linear(d_out, d_out, bias=False)\n",
    "        self.layer_norm = nn.LayerNorm(d_out)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        embed1 = self.linear1(x)\n",
    "        embed2 = self.drop(self.linear2(F.gelu(embed1)))\n",
    "        embeds = self.layer_norm(embed1 + embed2)\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, d_out: int) -> None:\n",
    "        super().__init__()\n",
    "        base = models.resnet34(pretrained=True)\n",
    "        d_in = base.fc.in_features\n",
    "        base.fc = nn.Identity()\n",
    "        self.base = base\n",
    "        self.projection = Projection(d_in, d_out)\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected_vec = self.projection(self.base(x))\n",
    "        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n",
    "        return projected_vec / projection_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, d_out: int) -> None:\n",
    "        super().__init__()\n",
    "        self.base = AutoModel.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "        self.projection = Projection(TRANSFORMER_EMBED, d_out)\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.base(x)[0]\n",
    "        out = out[:, 0, :]  # get CLS token output\n",
    "        projected_vec = self.projection(out)\n",
    "        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n",
    "        return projected_vec / projection_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, tokenizer: BertTokenizer) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, x: str) -> AutoTokenizer:\n",
    "        return self.tokenizer(\n",
    "            x, truncation=True, padding=True, return_tensors=\"pt\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(similarity: torch.Tensor):\n",
    "    y = torch.arange(len(similarity)).to(similarity.device)\n",
    "    img2cap_match_idx = similarity.argmax(dim=1)\n",
    "    cap2img_match_idx = similarity.argmax(dim=0)\n",
    "\n",
    "    img_acc = (img2cap_match_idx == y).float().mean()\n",
    "    cap_acc = (cap2img_match_idx == y).float().mean()\n",
    "\n",
    "    return img_acc, cap_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, lr: float = 1e-3) -> None:\n",
    "        super().__init__()\n",
    "        self.vision_encoder = VisionEncoder(EMBED_DIM)\n",
    "        self.caption_encoder = TextEncoder(EMBED_DIM)\n",
    "        self.tokenizer = Tokenizer(AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\"))\n",
    "        self.lr = lr\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def forward(self, images, text):\n",
    "        text = self.tokenizer(text).to(self.device)\n",
    "\n",
    "        image_embed = self.vision_encoder(images)\n",
    "        caption_embed = self.caption_encoder(text[\"input_ids\"])\n",
    "\n",
    "        similarity = caption_embed @ image_embed.T\n",
    "\n",
    "        loss = self.CLIP_loss(similarity)\n",
    "        img_acc, cap_acc = metrics(similarity)\n",
    "\n",
    "        return loss, img_acc, cap_acc\n",
    "    \n",
    "    def CLIP_loss(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        n = logits.shape[1]      # number of samples\n",
    "        labels = torch.arange(n).to(self.device) # Create labels tensor\n",
    "        # Calculate cross entropy losses along axis 0 and 1\n",
    "        loss_i = F.cross_entropy(logits.transpose(0, 1), labels, reduction=\"mean\")\n",
    "        loss_t = F.cross_entropy(logits, labels, reduction=\"mean\")\n",
    "        # Calculate the final loss\n",
    "        loss = (loss_i + loss_t) / 2\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def top_image(self, images, text):\n",
    "        text = self.tokenizer(text).to(self.device)\n",
    "        caption_embed = self.caption_encoder(text[\"input_ids\"])\n",
    "\n",
    "        similarities = []\n",
    "\n",
    "        for image in images:\n",
    "            image_embed = self.vision_encoder(image.to(self.device))\n",
    "            similarities.append(F.cosine_similarity(image_embed, caption_embed, dim=1).item())\n",
    "\n",
    "        #top_image = np.argsort(similarities)[-1:][::-1]\n",
    "\n",
    "        return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CustomModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.vision_encoder.parameters()},\n",
    "    {'params': model.caption_encoder.parameters()}\n",
    "], lr=model.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as tt\n",
    "\n",
    "scale = tt.Resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "tensor = tt.ToTensor()\n",
    "image_composed = tt.transforms.Compose([scale, tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = wsd.VisualWSDDataset(mode=\"train\", image_transform=image_composed)\n",
    "test_set = wsd.VisualWSDDataset(mode=\"test\", image_transform=image_composed, test_lang='en')\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=3)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1], Batch Loss: 3.4619855880737305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruwen/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Batch Loss: 2.6944098472595215\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "num_epochs = 1\n",
    "\n",
    "batch_zero = True\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    for batch in test_loader:\n",
    "        image = batch[\"correct_img\"].to(device)\n",
    "        text = batch[\"label_context\"]\n",
    "        # images, text = batch\n",
    "        loss, img_acc, cap_acc = model(image, text)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_zero:\n",
    "          print(f\"Epoch [{0}/{num_epochs}], Batch Loss: {loss.item()}\")\n",
    "          batch_zero = False\n",
    "\n",
    "\n",
    "    # Print training statistics\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Batch Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "stamp = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "\n",
    "path = \"./results/\" + stamp\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if k is 1 gives all instances with the correct prediction as top prediction\n",
    "# if k > 1 the correct prediction is in the top k predictions of the model\n",
    "def hit(results, k):\n",
    "    counter = 0\n",
    "\n",
    "    for r in results:\n",
    "        sims = np.absolute(r[1])\n",
    "        sorted = np.argsort(sims)[:k]\n",
    "\n",
    "        if r[0] in sorted:\n",
    "            counter += 1\n",
    "\n",
    "    return counter / len(results)\n",
    "\n",
    "def mrr(results):\n",
    "    sum = 0\n",
    "\n",
    "    for r in results:\n",
    "        sims = np.absolute(r[1])\n",
    "        sorted = np.argsort(sims)\n",
    "        sum += 1/(np.where(sorted==r[0])[0][0]+1)\n",
    "\n",
    "    return sum / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (vision_encoder): VisionEncoder(\n",
       "    (base): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Identity()\n",
       "    )\n",
       "    (projection): Projection(\n",
       "      (linear1): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (linear2): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (caption_encoder): TextEncoder(\n",
       "    (base): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0-5): 6 x TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (projection): Projection(\n",
       "      (linear1): Linear(in_features=768, out_features=512, bias=False)\n",
       "      (linear2): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model = CustomModel().to(device)\n",
    "model_name = \"2024-06-12-15:44:12\"\n",
    "eval_model.load_state_dict(torch.load(\"./results/\" + model_name))\n",
    "eval_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(loader):\n",
    "    results = []\n",
    "\n",
    "    for i,batch in enumerate(test_loader):\n",
    "        images = batch[\"imgs\"]\n",
    "        text = batch[\"label_context\"]\n",
    "        correct_idx = batch[\"correct_idx\"].item()\n",
    "        sims = eval_model.top_image(images, text)\n",
    "\n",
    "        results.append((correct_idx, sims))\n",
    "        top_image = np.argsort(np.absolute(sims))[0]\n",
    "\n",
    "        #print(np.argsort(np.absolute(sims)))\n",
    "        print(\"batch: \" + str(i+1) + \"/\" + str(len(test_loader)) + \" predicted: \" + str(top_image) + \" correct: \" + str(correct_idx))\n",
    "\n",
    "        if i == 50:\n",
    "            break\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 7 3 1 9 6 5 2 8 4]\n",
      "batch: 1/463 predicted: 0 correct: 8\n",
      "[3 7 4 0 5 2 9 6 1 8]\n",
      "batch: 2/463 predicted: 3 correct: 0\n",
      "[8 4 0 7 9 3 1 6 2 5]\n",
      "batch: 3/463 predicted: 8 correct: 5\n",
      "[7 5 0 3 1 2 8 6 9 4]\n",
      "batch: 4/463 predicted: 7 correct: 6\n",
      "[8 3 0 1 6 2 7 4 5 9]\n",
      "batch: 5/463 predicted: 8 correct: 2\n",
      "[9 5 0 2 3 8 6 4 7 1]\n",
      "batch: 6/463 predicted: 9 correct: 6\n",
      "[7 9 2 5 3 8 4 1 6 0]\n",
      "batch: 7/463 predicted: 7 correct: 9\n",
      "[2 6 7 1 0 3 9 4 8 5]\n",
      "batch: 8/463 predicted: 2 correct: 7\n",
      "[9 8 0 4 6 5 1 7 3 2]\n",
      "batch: 9/463 predicted: 9 correct: 2\n",
      "[5 8 3 6 9 4 0 7 2 1]\n",
      "batch: 10/463 predicted: 5 correct: 9\n",
      "[3 2 5 1 0 8 4 7 6 9]\n",
      "batch: 11/463 predicted: 3 correct: 4\n",
      "[0 2 3 4 5 6 1 9 8 7]\n",
      "batch: 12/463 predicted: 0 correct: 2\n",
      "[9 5 0 2 8 3 6 1 4 7]\n",
      "batch: 13/463 predicted: 9 correct: 5\n",
      "[0 6 2 1 4 3 8 7 9 5]\n",
      "batch: 14/463 predicted: 0 correct: 6\n",
      "[3 1 8 5 0 2 4 6 9 7]\n",
      "batch: 15/463 predicted: 3 correct: 4\n",
      "[5 2 8 7 9 0 1 6 4 3]\n",
      "batch: 16/463 predicted: 5 correct: 7\n",
      "[0 5 3 8 4 7 9 2 6 1]\n",
      "batch: 17/463 predicted: 0 correct: 0\n",
      "[8 5 6 0 9 2 3 1 7 4]\n",
      "batch: 18/463 predicted: 8 correct: 7\n",
      "[3 5 7 6 9 1 0 8 4 2]\n",
      "batch: 19/463 predicted: 3 correct: 1\n",
      "[0 3 8 1 7 4 2 9 5 6]\n",
      "batch: 20/463 predicted: 0 correct: 7\n",
      "[8 7 1 3 0 9 2 5 6 4]\n",
      "batch: 21/463 predicted: 8 correct: 9\n",
      "[6 7 3 1 5 4 0 8 2 9]\n",
      "batch: 22/463 predicted: 6 correct: 4\n",
      "[0 2 8 1 4 7 6 9 3 5]\n",
      "batch: 23/463 predicted: 0 correct: 9\n",
      "[1 7 3 5 0 4 6 8 9 2]\n",
      "batch: 24/463 predicted: 1 correct: 1\n",
      "[0 6 8 9 4 2 7 5 3 1]\n",
      "batch: 25/463 predicted: 0 correct: 8\n",
      "[2 5 9 6 4 0 7 1 8 3]\n",
      "batch: 26/463 predicted: 2 correct: 0\n",
      "[8 1 2 7 4 5 3 0 9 6]\n",
      "batch: 27/463 predicted: 8 correct: 7\n",
      "[7 2 0 6 5 8 9 1 3 4]\n",
      "batch: 28/463 predicted: 7 correct: 8\n",
      "[9 3 6 5 0 4 1 2 7 8]\n",
      "batch: 29/463 predicted: 9 correct: 2\n",
      "[0 5 2 7 9 6 1 4 8 3]\n",
      "batch: 30/463 predicted: 0 correct: 2\n",
      "[9 5 1 2 7 6 0 4 8 3]\n",
      "batch: 31/463 predicted: 9 correct: 4\n",
      "[8 5 0 4 7 1 9 2 3 6]\n",
      "batch: 32/463 predicted: 8 correct: 4\n",
      "[9 1 0 2 4 5 8 3 7 6]\n",
      "batch: 33/463 predicted: 9 correct: 4\n",
      "[8 5 6 2 3 9 4 7 1 0]\n",
      "batch: 34/463 predicted: 8 correct: 0\n",
      "[0 4 7 2 3 8 5 1 9 6]\n",
      "batch: 35/463 predicted: 0 correct: 2\n",
      "[7 9 8 3 5 0 6 1 4 2]\n",
      "batch: 36/463 predicted: 7 correct: 2\n",
      "[1 0 7 5 3 6 8 2 4 9]\n",
      "batch: 37/463 predicted: 1 correct: 1\n",
      "[0 8 7 3 2 5 4 9 6 1]\n",
      "batch: 38/463 predicted: 0 correct: 1\n",
      "[2 3 0 9 4 6 1 7 5 8]\n",
      "batch: 39/463 predicted: 2 correct: 0\n",
      "[7 9 2 5 8 0 3 1 6 4]\n",
      "batch: 40/463 predicted: 7 correct: 7\n",
      "[9 1 5 7 0 2 6 3 8 4]\n",
      "batch: 41/463 predicted: 9 correct: 7\n",
      "[4 5 2 8 9 7 1 6 0 3]\n",
      "batch: 42/463 predicted: 4 correct: 8\n",
      "[1 3 6 7 5 9 0 4 8 2]\n",
      "batch: 43/463 predicted: 1 correct: 3\n",
      "[5 6 9 4 1 2 0 3 8 7]\n",
      "batch: 44/463 predicted: 5 correct: 3\n",
      "[4 3 9 2 5 1 8 0 7 6]\n",
      "batch: 45/463 predicted: 4 correct: 0\n",
      "[7 5 8 6 2 9 1 4 0 3]\n",
      "batch: 46/463 predicted: 7 correct: 2\n",
      "[4 9 7 1 0 5 3 6 8 2]\n",
      "batch: 47/463 predicted: 4 correct: 6\n",
      "[8 5 4 6 2 7 3 0 1 9]\n",
      "batch: 48/463 predicted: 8 correct: 8\n",
      "[3 5 9 8 1 7 4 2 0 6]\n",
      "batch: 49/463 predicted: 3 correct: 7\n",
      "[4 8 2 6 1 5 9 3 7 0]\n",
      "batch: 50/463 predicted: 4 correct: 4\n",
      "[3 1 7 6 8 9 2 0 5 4]\n",
      "batch: 51/463 predicted: 3 correct: 6\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False)\n",
    "\n",
    "results = test_loop(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit@1: 0.11764705882352941\n",
      "mrr: 0.31030967942732646\n"
     ]
    }
   ],
   "source": [
    "print(\"hit@1: \" + str(hit(results, 1)))\n",
    "print(\"mrr: \" + str(mrr(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "bert_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokens(texts):\n",
    "    return bert_tokenizer(texts, truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_token = wsd.VisualWSDDataset(mode=\"train\", image_transform=image_composed, tokenizer=tokens)\n",
    "trial_set_token = wsd.VisualWSDDataset(mode=\"val\", image_transform=image_composed, tokenizer=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruwen/ki-lab-ss23-MindCraft/dataset.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['images'] = torch.tensor(correct_image).detach()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1998, 21716, 11960,  3392,   102,     0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 0, 0]),\n",
       " 'images': tensor([[[0.0510, 0.0118, 0.0510,  ..., 0.3059, 0.3059, 0.3216],\n",
       "          [0.1059, 0.0431, 0.0588,  ..., 0.2510, 0.2471, 0.2667],\n",
       "          [0.0667, 0.0510, 0.0784,  ..., 0.1490, 0.1294, 0.1804],\n",
       "          ...,\n",
       "          [0.6471, 0.5412, 0.4784,  ..., 0.1255, 0.1333, 0.1882],\n",
       "          [0.5922, 0.4941, 0.4706,  ..., 0.1569, 0.1412, 0.2275],\n",
       "          [0.5725, 0.5059, 0.4941,  ..., 0.1608, 0.1725, 0.3255]],\n",
       " \n",
       "         [[0.1059, 0.0863, 0.0706,  ..., 0.3176, 0.3294, 0.3451],\n",
       "          [0.1294, 0.0980, 0.0588,  ..., 0.2510, 0.2627, 0.2941],\n",
       "          [0.1333, 0.0980, 0.0431,  ..., 0.1451, 0.1490, 0.2039],\n",
       "          ...,\n",
       "          [0.6627, 0.5529, 0.4667,  ..., 0.2706, 0.3059, 0.3020],\n",
       "          [0.6078, 0.4980, 0.4706,  ..., 0.3137, 0.2863, 0.2510],\n",
       "          [0.5843, 0.5059, 0.4980,  ..., 0.3137, 0.2627, 0.2235]],\n",
       " \n",
       "         [[0.0235, 0.0039, 0.0000,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0471, 0.0039, 0.0000,  ..., 0.0039, 0.0039, 0.0078],\n",
       "          [0.0314, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0118],\n",
       "          ...,\n",
       "          [0.5529, 0.4353, 0.3569,  ..., 0.0706, 0.0314, 0.0706],\n",
       "          [0.5020, 0.3843, 0.3608,  ..., 0.1020, 0.0196, 0.0471],\n",
       "          [0.4980, 0.4078, 0.4000,  ..., 0.0941, 0.0275, 0.0627]]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_set_token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModelHugging(nn.Module):\n",
    "    def __init__(self, lr: float = 1e-3) -> None:\n",
    "        super().__init__()\n",
    "        self.vision_encoder = VisionEncoder(EMBED_DIM)\n",
    "        self.caption_encoder = TextEncoder(EMBED_DIM)\n",
    "        #self.tokenizer = Tokenizer(AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\"))\n",
    "        self.lr = lr\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        image_embed = self.vision_encoder(images)\n",
    "        caption_embed = self.caption_encoder(input_ids)\n",
    "\n",
    "        similarity = caption_embed @ image_embed.T\n",
    "\n",
    "        loss = self.CLIP_loss(similarity)\n",
    "        img_acc, cap_acc = metrics(similarity)\n",
    "\n",
    "        return loss, img_acc, cap_acc\n",
    "    \n",
    "    def CLIP_loss(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        n = logits.shape[1]      # number of samples\n",
    "        labels = torch.arange(n).to(self.device) # Create labels tensor\n",
    "        # Calculate cross entropy losses along axis 0 and 1\n",
    "        loss_i = F.cross_entropy(logits.transpose(0, 1), labels, reduction=\"mean\")\n",
    "        loss_t = F.cross_entropy(logits, labels, reduction=\"mean\")\n",
    "        # Calculate the final loss\n",
    "        loss = (loss_i + loss_t) / 2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/ruwen/ki-lab-ss23-MindCraft/dataset.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['images'] = torch.tensor(correct_image).detach()\n",
      "/home/ruwen/ki-lab-ss23-MindCraft/dataset.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['images'] = torch.tensor(correct_image).detach()\n",
      "/home/ruwen/ki-lab-ss23-MindCraft/dataset.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['images'] = torch.tensor(correct_image).detach()\n",
      "/home/ruwen/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='606' max='606' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [606/606 1:41:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.158900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.159300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>4.159600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>4.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>4.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>4.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>4.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>4.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>4.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>4.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>4.157600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.904300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>4.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>4.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>4.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.158300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>4.158900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>4.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>4.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>4.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>4.159800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>4.159300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>4.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>4.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>4.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>4.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>4.159800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>4.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.160200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>3.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>4.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>4.159800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>4.158900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.159700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>4.159800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>4.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>4.158700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>4.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>4.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>4.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>4.159800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>4.159600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.157600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>4.158900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>4.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>4.159700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>4.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.158400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (102080000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (141880000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (178315136 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (148635000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (89550550 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:890: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (104405838 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:890: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (131300000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/ki-lab-ss23-MindCraft/dataset.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['images'] = torch.tensor(correct_image).detach()\n",
      "/home/ruwen/ki-lab-ss23-MindCraft/dataset.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['images'] = torch.tensor(correct_image).detach()\n",
      "/home/ruwen/ki-lab-ss23-MindCraft/dataset.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['images'] = torch.tensor(correct_image).detach()\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (141880000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (89550550 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (104405838 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:890: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (178315136 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (102080000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (148635000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:890: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (131300000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/ki-lab-ss23-MindCraft/dataset.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['images'] = torch.tensor(correct_image).detach()\n",
      "/home/ruwen/ki-lab-ss23-MindCraft/dataset.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['images'] = torch.tensor(correct_image).detach()\n",
      "/home/ruwen/ki-lab-ss23-MindCraft/dataset.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['images'] = torch.tensor(correct_image).detach()\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (104405838 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:890: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (141880000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (131300000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (89550550 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (148635000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (102080000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:890: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/ruwen/miniconda3/envs/mindcraft_wsd/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (178315136 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=606, training_loss=4.146454820538511, metrics={'train_runtime': 6095.2045, 'train_samples_per_second': 6.334, 'train_steps_per_second': 0.099, 'total_flos': 0.0, 'train_loss': 4.146454820538511, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=300,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    dataloader_num_workers=3,\n",
    ")\n",
    "\n",
    "model = CustomModelHugging()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated  Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_set_token,         # training dataset\n",
    "    eval_dataset=trial_set_token,             # evaluation dataset\n",
    "    tokenizer=bert_tokenizer,\n",
    "    optimizers=(optimizer, None)        # no scheduler\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Trainer ausprobieren\n",
    "# DataLoader anpassen mit Validation Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ki-labor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
